version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.1
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    ports:
      - "9092:9092"

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      CLUSTER_NAME: smartcity
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_replication: 1
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./hadoop/conf:/etc/hadoop/conf
    ports:
      - "9870:9870"
      - "8020:8020"

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    depends_on:
      - namenode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HDFS_CONF_dfs_replication: 1
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
      SERVICE_PRECONDITION: namenode:9870
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./hadoop/conf:/etc/hadoop/conf
    ports:
      - "9864:9864"

  spark-master:
    image: apache/spark:3.5.1
    depends_on:
      - namenode
      - datanode
      - kafka
    environment:
      HADOOP_CONF_DIR: /opt/spark/conf/hadoop
    volumes:
      - ../spark:/opt/spark/jobs
      - ./hadoop/conf:/opt/spark/conf/hadoop
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master"]
    ports:
      - "7077:7077"
      - "8080:8080"

  spark-worker:
    image: apache/spark:3.5.1
    depends_on:
      - spark-master
    environment:
      HADOOP_CONF_DIR: /opt/spark/conf/hadoop
    volumes:
      - ../spark:/opt/spark/jobs
      - ./hadoop/conf:/opt/spark/conf/hadoop
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--webui-port", "8081"]
    ports:
      - "8081:8081"

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"

  airflow-init:
    build:
      context: ..
      dockerfile: docker/airflow/Dockerfile
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      HADOOP_CONF_DIR: /opt/hadoop/conf
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../generator:/opt/airflow/project/generator
      - ../spark:/opt/airflow/project/spark
      - ./hadoop/conf:/opt/hadoop/conf
      - airflow_logs:/opt/airflow/logs
    command: ["bash", "-c", "airflow db migrate && airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com"]

  airflow-webserver:
    build:
      context: ..
      dockerfile: docker/airflow/Dockerfile
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      KAFKA_PARTITIONS: ${KAFKA_PARTITIONS}
      KAFKA_REPLICATION_FACTOR: ${KAFKA_REPLICATION_FACTOR}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      TRAFFIC_DB: ${TRAFFIC_DB}
      TRAFFIC_DB_USER: ${TRAFFIC_DB_USER}
      TRAFFIC_DB_PASSWORD: ${TRAFFIC_DB_PASSWORD}
      HADOOP_CONF_DIR: /opt/hadoop/conf
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../generator:/opt/airflow/project/generator
      - ../spark:/opt/airflow/project/spark
      - ./hadoop/conf:/opt/hadoop/conf
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8082:8080"
    command: ["bash", "-c", "airflow webserver"]

  airflow-scheduler:
    build:
      context: ..
      dockerfile: docker/airflow/Dockerfile
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      KAFKA_PARTITIONS: ${KAFKA_PARTITIONS}
      KAFKA_REPLICATION_FACTOR: ${KAFKA_REPLICATION_FACTOR}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      TRAFFIC_DB: ${TRAFFIC_DB}
      TRAFFIC_DB_USER: ${TRAFFIC_DB_USER}
      TRAFFIC_DB_PASSWORD: ${TRAFFIC_DB_PASSWORD}
      HADOOP_CONF_DIR: /opt/hadoop/conf
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../generator:/opt/airflow/project/generator
      - ../spark:/opt/airflow/project/spark
      - ./hadoop/conf:/opt/hadoop/conf
      - airflow_logs:/opt/airflow/logs
    command: ["bash", "-c", "airflow scheduler"]

  grafana:
    image: grafana/grafana:10.4.3
    depends_on:
      - postgres
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_NEWS_NEWS_FEED_ENABLED: "false"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
      GF_SECURITY_DISABLE_GRAVATAR: "true"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3000:3000"

volumes:
  namenode_data:
  datanode_data:
  postgres_data:
  airflow_logs:
  grafana_data:
